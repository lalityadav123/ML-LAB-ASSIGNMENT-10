{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Amol Bansal ML lab Assignment 10 .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh0x1EQv1I4Y",
        "colab_type": "text"
      },
      "source": [
        "  \n",
        "*   Name : Lalit Yadav\n",
        "*   ID NO. : 1700230C203\n",
        "*   SEC : CSE-2\n",
        "     \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## PROBLEM 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKmgu6KUtFZr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "87b74a4c-2981-4643-8df9-e609c512845a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "dice=0\n",
        "for i in range(250):\n",
        "    walk = [0] \n",
        "    for x in range(100):\n",
        "        step = walk[-1] \n",
        "        dice = random.randint(1,7)\n",
        "        if dice <= 2 :\n",
        "            step = max(0, step - 1)\n",
        "\n",
        "        elif dice<=5:\n",
        "            step += 1\n",
        "        else:\n",
        "            step = step + random.randint(1,7)\n",
        "    print(step)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "1\n",
            "7\n",
            "1\n",
            "3\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "6\n",
            "0\n",
            "1\n",
            "7\n",
            "3\n",
            "3\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "4\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "3\n",
            "1\n",
            "0\n",
            "3\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "0\n",
            "0\n",
            "7\n",
            "5\n",
            "1\n",
            "3\n",
            "5\n",
            "4\n",
            "1\n",
            "4\n",
            "1\n",
            "6\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "4\n",
            "1\n",
            "4\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "3\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "7\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "4\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "4\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "6\n",
            "2\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "2\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "7\n",
            "7\n",
            "6\n",
            "1\n",
            "1\n",
            "3\n",
            "4\n",
            "3\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "3\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "7\n",
            "0\n",
            "4\n",
            "1\n",
            "7\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "6\n",
            "1\n",
            "4\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "5\n",
            "3\n",
            "4\n",
            "3\n",
            "1\n",
            "0\n",
            "7\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "5\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "6\n",
            "0\n",
            "6\n",
            "0\n",
            "1\n",
            "1\n",
            "2\n",
            "0\n",
            "1\n",
            "1\n",
            "6\n",
            "7\n",
            "1\n",
            "1\n",
            "6\n",
            "0\n",
            "2\n",
            "4\n",
            "1\n",
            "1\n",
            "1\n",
            "6\n",
            "1\n",
            "0\n",
            "4\n",
            "0\n",
            "5\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Tx5eJxO1WlR",
        "colab_type": "text"
      },
      "source": [
        "## PROBLEM 2\n",
        "\n",
        "Generation of Random data for Mutliple linear regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ld2Dm1Ztht6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "91341cca-8850-4add-b3bc-56951d51d107"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import random\n",
        "from scipy.stats import norm\n",
        "random.seed(1)\n",
        "n= 3\n",
        "X=[]\n",
        "for i in range(0,n):\n",
        "    X_i= scipy.stats.norm.rvs(0, 1, 100)\n",
        "    X.append(X_i)\n",
        "eps=scipy.stats.norm.rvs(0, 1, 100)\n",
        "y = 1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2])  + eps\n",
        "data_ols = {'X0': X[0],'X1':X[1],'X2':X[2] ,'Y': y }\n",
        "df = pd.DataFrame(data_ols)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.info())\n",
        "print(df.describe())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         X0        X1        X2         Y\n",
            "0  1.295365 -2.115902  0.791426  0.672025\n",
            "1  0.480073 -1.562138  1.201919  0.696396\n",
            "2 -1.872989  0.600965 -0.365426  0.366717\n",
            "3 -0.739838  1.728669 -1.494665 -1.110818\n",
            "4  1.288854 -1.657786 -0.551325 -0.278165\n",
            "          X0        X1        X2         Y\n",
            "95 -1.184047 -0.173689  0.026561 -0.902720\n",
            "96 -0.165695 -0.405795  0.409227  1.667752\n",
            "97 -0.556497 -0.491792 -0.060667  0.627619\n",
            "98 -0.756935 -0.491884 -0.065522  0.902168\n",
            "99  0.248078 -0.402047 -2.668386  0.779664\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 4 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   Y       100 non-null    float64\n",
            "dtypes: float64(4)\n",
            "memory usage: 3.2 KB\n",
            "None\n",
            "               X0          X1          X2           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000\n",
            "mean    -0.052931   -0.157578    0.061213    0.712976\n",
            "std      1.008576    1.035371    0.982450    1.279860\n",
            "min     -2.760081   -2.890591   -2.668386   -2.463379\n",
            "25%     -0.762919   -0.692226   -0.524661   -0.065475\n",
            "50%      0.005372   -0.134106    0.024867    0.627163\n",
            "75%      0.846226    0.387855    0.632701    1.447771\n",
            "max      1.818336    2.532007    2.903425    4.059816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq5CXxGS1kWR",
        "colab_type": "text"
      },
      "source": [
        " Data generation for logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2pIf7Bhv6Ke",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "a979e8e3-53f4-4dbe-8f73-aa546fabdb9b"
      },
      "source": [
        "X = []\n",
        "n = 3\n",
        "for i in range(0,n):\n",
        "  X_i = scipy.stats.norm.rvs(0, 1, 100)\n",
        "  X.append(X_i)\n",
        "odds = (np.exp(1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2])) /(1 + np.exp(1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2]) ))) \n",
        "y1 = [ ]\n",
        "for i in odds:\n",
        "  if (i>=0.5):\n",
        "    y1.append(1)\n",
        "  else:\n",
        "    y1.append(0)\n",
        "data_lr = {'X0': X[0],'X1':X[1],'X2':X[2] ,'Y': y1 }\n",
        "df1 = pd.DataFrame(data_lr)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.info())\n",
        "print(df.describe())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         X0        X1        X2         Y\n",
            "0  1.295365 -2.115902  0.791426  0.672025\n",
            "1  0.480073 -1.562138  1.201919  0.696396\n",
            "2 -1.872989  0.600965 -0.365426  0.366717\n",
            "3 -0.739838  1.728669 -1.494665 -1.110818\n",
            "4  1.288854 -1.657786 -0.551325 -0.278165\n",
            "          X0        X1        X2         Y\n",
            "95 -1.184047 -0.173689  0.026561 -0.902720\n",
            "96 -0.165695 -0.405795  0.409227  1.667752\n",
            "97 -0.556497 -0.491792 -0.060667  0.627619\n",
            "98 -0.756935 -0.491884 -0.065522  0.902168\n",
            "99  0.248078 -0.402047 -2.668386  0.779664\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 4 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   Y       100 non-null    float64\n",
            "dtypes: float64(4)\n",
            "memory usage: 3.2 KB\n",
            "None\n",
            "               X0          X1          X2           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000\n",
            "mean    -0.052931   -0.157578    0.061213    0.712976\n",
            "std      1.008576    1.035371    0.982450    1.279860\n",
            "min     -2.760081   -2.890591   -2.668386   -2.463379\n",
            "25%     -0.762919   -0.692226   -0.524661   -0.065475\n",
            "50%      0.005372   -0.134106    0.024867    0.627163\n",
            "75%      0.846226    0.387855    0.632701    1.447771\n",
            "max      1.818336    2.532007    2.903425    4.059816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SpzYmmO2GCu",
        "colab_type": "text"
      },
      "source": [
        "Data generation for K-means"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUf7vSPvv8u_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "outputId": "062e353a-e5b3-4e0e-f5a0-ee83c7be9a1c"
      },
      "source": [
        "X_a= -2 * np.random.rand(100,2)\n",
        "X_b = 1 + 2 * np.random.rand(50,2)\n",
        "X_a[50:100, :] = X_b\n",
        "plt.scatter(X_a[ : , 0], X_a[ :, 1], s = 50)\n",
        "plt.show()\n",
        "data_kmeans = {'X0': X_a[:,0],'X1':X_a[:,1]}\n",
        "df3 = pd.DataFrame(data_kmeans)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.info())\n",
        "print(df.describe())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "         X0        X1        X2         Y\n",
            "0  1.295365 -2.115902  0.791426  0.672025\n",
            "1  0.480073 -1.562138  1.201919  0.696396\n",
            "2 -1.872989  0.600965 -0.365426  0.366717\n",
            "3 -0.739838  1.728669 -1.494665 -1.110818\n",
            "4  1.288854 -1.657786 -0.551325 -0.278165\n",
            "          X0        X1        X2         Y\n",
            "95 -1.184047 -0.173689  0.026561 -0.902720\n",
            "96 -0.165695 -0.405795  0.409227  1.667752\n",
            "97 -0.556497 -0.491792 -0.060667  0.627619\n",
            "98 -0.756935 -0.491884 -0.065522  0.902168\n",
            "99  0.248078 -0.402047 -2.668386  0.779664\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 4 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   Y       100 non-null    float64\n",
            "dtypes: float64(4)\n",
            "memory usage: 3.2 KB\n",
            "None\n",
            "               X0          X1          X2           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000\n",
            "mean    -0.052931   -0.157578    0.061213    0.712976\n",
            "std      1.008576    1.035371    0.982450    1.279860\n",
            "min     -2.760081   -2.890591   -2.668386   -2.463379\n",
            "25%     -0.762919   -0.692226   -0.524661   -0.065475\n",
            "50%      0.005372   -0.134106    0.024867    0.627163\n",
            "75%      0.846226    0.387855    0.632701    1.447771\n",
            "max      1.818336    2.532007    2.903425    4.059816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxKLMLb22vze",
        "colab_type": "text"
      },
      "source": [
        "## PROBLEM 3\n",
        "\n",
        "a. Linear Regression using Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuLpe_VAwWTZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4509bb2-7a49-462a-827f-fcfa7fce6646"
      },
      "source": [
        "X = df.iloc[:,0].values\n",
        "#print(X)\n",
        "y = df.iloc[:,3].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 150\n",
        " \n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2\n",
        "  d1 = (-2/n) * sum(X * (y - y_p))\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "\n",
        "print(b1,b0)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.10112485691699877 0.18570497231751598\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evUnEJI03Bha",
        "colab_type": "text"
      },
      "source": [
        "b. Logistic Regression using Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEMit1GJxMJw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "600fadf3-beb6-405c-a37b-19ff136dc14c"
      },
      "source": [
        "X1 = df1.iloc[:,0:3].values\n",
        "y1 = df1.iloc[:,3].values\n",
        "\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat)))\n",
        "\n",
        "W = np.zeros((3,1))\n",
        "b = np.zeros((1,1))\n",
        "\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz)\n",
        "  db = np.sum(dz)\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6931471805599453\n",
            "0.2786949100074348\n",
            "0.27862178692311973\n",
            "0.278549979697829\n",
            "0.27847946632558684\n",
            "0.2784102253067385\n",
            "0.27834223562565324\n",
            "0.2782754767294901\n",
            "0.2782099285079885\n",
            "0.27814557127424605\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUpVDAuc3G_z",
        "colab_type": "text"
      },
      "source": [
        "c. Linear Regression using L1 Regulrisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAEFofTExSFP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "01e06195-d3fc-43d4-ba1c-6129fc3cf367"
      },
      "source": [
        "X = df.iloc[:,0].values\n",
        "y = df.iloc[:,3].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        "lam = 0.1\n",
        " \n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2 + (lam * b1)\n",
        "  d1 = (-2/n) * sum(X * (y - y_p)) + lam\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "\n",
        "print(b1,b0)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0613818596473699 0.12966796542615613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZisYlDuV3SIM",
        "colab_type": "text"
      },
      "source": [
        "c. Linear Regression using L2 Regulrisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9novOSUbxkeq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ec219845-72f3-46c3-ed77-416ada0dc4c8"
      },
      "source": [
        "X = df.iloc[:,0].values\n",
        "#print(X)\n",
        "y = df.iloc[:,3].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        "lam = 0.1\n",
        " \n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2 + ((lam/2) * b1)\n",
        "  d1 = (-2/n) * sum(X * (y - y_p)) + (lam *b1)\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "  print(b1,b0)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0007700236145829261 0.0014259521020503652\n",
            "0.0015385659569517596 0.002849133816476247\n",
            "0.002305629873694468 0.004269550527241802\n",
            "0.0030712182059350374 0.005687207607844605\n",
            "0.003835333789343944 0.0071021104213360026\n",
            "0.004597979454148611 0.008514264320341433\n",
            "0.005359158025143843 0.009923674647080695\n",
            "0.006118872321702234 0.01133034673338819\n",
            "0.0068771251577845706 0.012734285900733115\n",
            "0.0076339193419501985 0.01413549746023962\n",
            "0.00838925767736738 0.015533986712706928\n",
            "0.00914314296182363 0.016929758948629418\n",
            "0.009895577987736028 0.018322819448216655\n",
            "0.010646565542161514 0.019713173481413403\n",
            "0.011396108406807163 0.021100826307919575\n",
            "0.012144209358040444 0.022485783177210165\n",
            "0.012890871166899453 0.023868049328555133\n",
            "0.013636096599103127 0.025247629991039244\n",
            "0.014379888415061447 0.026624530383581882\n",
            "0.01512224936988561 0.027998755714956813\n",
            "0.01586318221339819 0.029370311183811917\n",
            "0.016602689690143276 0.030739201978688888\n",
            "0.01734077453939659 0.032105433278042876\n",
            "0.018077439495175585 0.03346901025026211\n",
            "0.01881268728624953 0.03482993805368746\n",
            "0.019546520636149565 0.03618822183663201\n",
            "0.020278942263178747 0.037543866737400525\n",
            "0.021009954880422076 0.03889687788430892\n",
            "0.021739561195756492 0.040247260395703706\n",
            "0.022467763911860852 0.04159501937998135\n",
            "0.023194565726225917 0.04294015993560765\n",
            "0.02391996933116428 0.044282687151137035\n",
            "0.024643977413820295 0.045622606105231826\n",
            "0.02536659265617999 0.046959921866681506\n",
            "0.026087817735080957 0.048294639494421886\n",
            "0.026807655322222214 0.04962676403755429\n",
            "0.027526108084174068 0.050956300535364665\n",
            "0.028243178682387935 0.052283254017342684\n",
            "0.02895886977320616 0.053607629503200784\n",
            "0.029673184007871813 0.054929432002893186\n",
            "0.03038612403253846 0.05624866651663486\n",
            "0.03109769248827992 0.05756533803492049\n",
            "0.031807892011100014 0.058879451538543355\n",
            "0.032516725231942274 0.060191011998614194\n",
            "0.033224194776699655 0.061500024376580045\n",
            "0.0339303032662242 0.06280649362424305\n",
            "0.034635053316336706 0.06411042468377917\n",
            "0.03533844753783641 0.06541182248775695\n",
            "0.036040488536510554 0.0667106919591562\n",
            "0.03674117891314404 0.06800703801138659\n",
            "0.037440521263529 0.0693008655483063\n",
            "0.03813851817847438 0.07059217946424062\n",
            "0.03883517224381547 0.07188098464400043\n",
            "0.039530486040423474 0.07316728596290074\n",
            "0.040224462144215004 0.07445108828677917\n",
            "0.040917103126161584 0.07573239647201431\n",
            "0.04160841155229913 0.0770112153655442\n",
            "0.04229838998373741 0.07828754980488464\n",
            "0.042987040976669506 0.07956140461814754\n",
            "0.04367436708238121 0.08083278462405914\n",
            "0.044360370847260466 0.08210169463197838\n",
            "0.045045054812806726 0.08336813944191498\n",
            "0.045728421515640355 0.08463212384454775\n",
            "0.04641047348751196 0.08589365262124266\n",
            "0.04709121325531175 0.08715273054407094\n",
            "0.04777064334107883 0.0884093623758272\n",
            "0.04844876626201052 0.08966355287004744\n",
            "0.04912558453047163 0.09091530677102709\n",
            "0.04980110065400373 0.09216462881383893\n",
            "0.05047531713533438 0.09341152372435103\n",
            "0.051148236472386394 0.09465599621924473\n",
            "0.051819861158287024 0.09589805100603238\n",
            "0.05249019368137717 0.09713769278307528\n",
            "0.05315923652522053 0.0983749262396014\n",
            "0.05382699216861281 0.09960975605572317\n",
            "0.05449346308559081 0.10084218690245524\n",
            "0.055158651745441586 0.1020722234417321\n",
            "0.05582256061271154 0.10329987032642579\n",
            "0.05648519214721552 0.10452513220036351\n",
            "0.05714654880404588 0.10574801369834522\n",
            "0.05780663303358153 0.10696851944616115\n",
            "0.05846544728149701 0.10818665406060939\n",
            "0.05912299398877147 0.1094024221495133\n",
            "0.059779275591697684 0.11061582831173905\n",
            "0.060434294521891044 0.11182687713721294\n",
            "0.061088053206298516 0.11303557320693886\n",
            "0.061740554067207604 0.1142419210930156\n",
            "0.06239179952225527 0.11544592535865422\n",
            "0.06304179198443688 0.11664759055819525\n",
            "0.06369053386211505 0.11784692123712599\n",
            "0.0643380275590286 0.11904392193209776\n",
            "0.06498427547430133 0.12023859717094298\n",
            "0.06562928000245097 0.12143095147269245\n",
            "0.06627304353339793 0.12262098934759236\n",
            "0.06691556845247415 0.1238087152971214\n",
            "0.06755685714043193 0.12499413381400784\n",
            "0.06819691197345262 0.12617724938224653\n",
            "0.0688357353231555 0.12735806647711587\n",
            "0.06947332955660641 0.12853658956519476\n",
            "0.07010969703632658 0.12971282310437954\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wr1dVbR3VRl",
        "colab_type": "text"
      },
      "source": [
        "d. Logistic regression using L1 regularisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vBpNasiyhLP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "9fcfd301-3fed-434a-8dad-ec2b19c3dfa8"
      },
      "source": [
        "X1 = df1.iloc[:,0:3].values\n",
        "y1 = df1.iloc[:,3].values\n",
        "lam = 0.1\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat))) + (lam * (np.sum(W)))\n",
        "\n",
        "W = np.zeros((3,1))\n",
        "b = np.zeros((1,1))\n",
        "\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz) + lam\n",
        "  db = np.sum(dz)\n",
        "\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6931471805599453\n",
            "-0.01956266244806354\n",
            "-0.31529380038254334\n",
            "-0.6081493350797085\n",
            "-0.8981554913328884\n",
            "-1.185338591294484\n",
            "-1.4697250474159105\n",
            "-1.7513413566647766\n",
            "-2.030214095746669\n",
            "-2.3063699170767826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ0xHITR3eBk",
        "colab_type": "text"
      },
      "source": [
        "d. Logistic regression using L2 regularisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-2nrupZyqVu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "592d6bf1-f901-4857-8bda-4d832485a6e5"
      },
      "source": [
        "X1 = df1.iloc[:,0:3].values\n",
        "y1 = df1.iloc[:,3].values\n",
        "lam = 0.1\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat))) + (lam * (np.sum(np.square(W))))\n",
        "\n",
        "W = np.zeros((3,1))\n",
        "b = np.zeros((1,1))\n",
        "\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz) + lam * W\n",
        "  db = np.sum(dz)\n",
        "\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6931471805599453\n",
            "0.27876901409046656\n",
            "0.27891262004626843\n",
            "0.27919208551444685\n",
            "0.2795996494127778\n",
            "0.2801278852768944\n",
            "0.28076968768352334\n",
            "0.28151825928910584\n",
            "0.2823670984479011\n",
            "0.28330998737622765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqfQ5etV3mxo",
        "colab_type": "text"
      },
      "source": [
        "e. K means clustering algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODIT9UTIy_VC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "e6bb3290-4277-4938-8289-072f5750f2d0"
      },
      "source": [
        "class K_Means:\n",
        "    def __init__(self, k=2, tol=0.001, max_iter=300):\n",
        "        self.k = k\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def fit(self,data):\n",
        "\n",
        "        self.centroids = {}\n",
        "\n",
        "        for i in range(self.k):\n",
        "            self.centroids[i] = data[i]\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            self.classifications = {}\n",
        "\n",
        "            for i in range(self.k):\n",
        "                self.classifications[i] = []\n",
        "\n",
        "            for featureset in X:\n",
        "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
        "                classification = distances.index(min(distances))\n",
        "                self.classifications[classification].append(featureset)\n",
        "\n",
        "            prev_centroids = dict(self.centroids)\n",
        "\n",
        "            for classification in self.classifications:\n",
        "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
        "\n",
        "            optimized = True\n",
        "\n",
        "            for c in self.centroids:\n",
        "                original_centroid = prev_centroids[c]\n",
        "                current_centroid = self.centroids[c]\n",
        "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
        "                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
        "                    optimized = False\n",
        "\n",
        "            if optimized:\n",
        "                break\n",
        "\n",
        "    def predict(self,data):\n",
        "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
        "        classification = distances.index(min(distances))\n",
        "        return classification\n",
        "        \n",
        "colors = 10*[\"g\",\"r\",\"c\",\"b\",\"k\"]\n",
        "\n",
        "X = df3.iloc[:,0:2].values\n",
        "clf = K_Means()\n",
        "clf.fit(X)\n",
        "\n",
        "for centroid in clf.centroids:\n",
        "    plt.scatter(clf.centroids[centroid][0], clf.centroids[centroid][1],\n",
        "                marker=\"o\", color=\"k\", s=150, linewidths=5)\n",
        "\n",
        "for classification in clf.classifications:\n",
        "    color = colors[classification]\n",
        "    for featureset in clf.classifications[classification]:\n",
        "        plt.scatter(featureset[0], featureset[1], marker=\"x\", color=color, s=150, linewidths=5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "252.14556376446103\n",
            "83.2549744173395\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFh9gABL3p6a",
        "colab_type": "text"
      },
      "source": [
        "## PROBLEM 4\n",
        "\n",
        "Linear Regression using OOPS\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clDecl_WzM3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class LinearRegressionModel():\n",
        "\n",
        "    def __init__(self, dataset, learning_rate, num_iterations):\n",
        "        self.dataset = np.array(dataset)\n",
        "        self.b = 0  \n",
        "        self.m = 0  \n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.M = len(self.dataset)\n",
        "        self.total_error = 0\n",
        "\n",
        "    def apply_gradient_descent(self):\n",
        "        for i in range(self.num_iterations):\n",
        "            self.do_gradient_step()\n",
        "\n",
        "    def do_gradient_step(self):\n",
        "        b_summation = 0\n",
        "        m_summation = 0\n",
        "        for i in range(self.M):\n",
        "            x_value = self.dataset[i, 0]\n",
        "            y_value = self.dataset[i, 1]\n",
        "            b_summation += (((self.m * x_value) + self.b) - y_value) \n",
        "            m_summation += (((self.m * x_value) + self.b) - y_value) * x_value\n",
        "        self.b = self.b - (self.learning_rate * (1/self.M) * b_summation)\n",
        "        self.m = self.m - (self.learning_rate * (1/self.M) * m_summation)\n",
        "      \n",
        "    def compute_error(self):\n",
        "        for i in range(self.M):\n",
        "            x_value = self.dataset[i, 0]\n",
        "            y_value = self.dataset[i, 1]\n",
        "            self.total_error += ((self.m * x_value) + self.b) - y_value\n",
        "        return self.total_error\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Results: b: {}, m: {}, Final Total error: {}\".format(round(self.b, 2), round(self.m, 2), round(self.compute_error(), 2))\n",
        "\n",
        "    def get_prediction_based_on(self, x):\n",
        "        return round(float((self.m * x) + self.b), 2) \n",
        "\n",
        "def main():\n",
        "    school_dataset = np.genfromtxt(DATASET_PATH, delimiter=\",\")\n",
        "    lr = LinearRegressionModel(school_dataset, 0.0001, 1000)\n",
        "    lr.apply_gradient_descent()\n",
        "    hours = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "    for hour in hours:\n",
        "        print(\"Studied {} hours and got {} points.\".format(hour, lr.get_prediction_based_on(hour)))\n",
        "    print(lr)\n",
        "    if __name__ == \"__main__\": main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMbArgbs30iR",
        "colab_type": "text"
      },
      "source": [
        "Logistic Regression using OOPS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vq2L9qczZim",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegression:\n",
        "  def __init__(self, learning_rate, num_iters, fit_intercept = True, verbose = False):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.num_iters = num_iters\n",
        "    self.fit_intercept = fit_intercept\n",
        "    self.verbose = verbose\n",
        "  def __add_intercept(self, X):\n",
        "    intercept = np.ones((X.shape[0],1))\n",
        "    return np.concatenate((intercept,X),axis=1)\n",
        "  def __sigmoid(self,z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "  def __loss(self, h, y):\n",
        "    return (-y * np.log(h) - (1-y) * np.log(1-h)).mean()\n",
        "  \n",
        "  def fit(self,X,y):\n",
        "    if self.fit_intercept:\n",
        "      X = self.__add_intercept(X)\n",
        "    self.theta = np.zeros(X.shape[1])\n",
        "    \n",
        "    for i in range(self.num_iters):\n",
        "      z = np.dot(X,self.theta)\n",
        "      h = self.__sigmoid(z)\n",
        "      gradient = np.dot(X.T,(h-y))/y.size\n",
        "      \n",
        "      self.theta -= self.learning_rate * gradient\n",
        "      \n",
        "      z = np.dot(X,self.theta)\n",
        "      h = self.__sigmoid(z)\n",
        "      loss = self.__loss(h,y)\n",
        "      \n",
        "      if self.verbose == True and i % 1000 == 0:\n",
        "        print(f'Loss: {loss}\\t')\n",
        "  def predict_probability(self,X):\n",
        "    if self.fit_intercept:\n",
        "      X = self.__add_intercept(X)\n",
        "    return self.__sigmoid(np.dot(X,self.theta))\n",
        "  def predict(self,X):\n",
        "    return (self.predict_probability(X).round())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqcTP8AS34_8",
        "colab_type": "text"
      },
      "source": [
        "K-means clustering using OOPS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng904l__0EUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class K_Means:\n",
        "    def __init__(self, k=2, tol=0.001, max_iter=300):\n",
        "        self.k = k\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def fit(self,data):\n",
        "\n",
        "        self.centroids = {}\n",
        "\n",
        "        for i in range(self.k):\n",
        "            self.centroids[i] = data[i]\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            self.classifications = {}\n",
        "\n",
        "            for i in range(self.k):\n",
        "                self.classifications[i] = []\n",
        "\n",
        "            for featureset in X:\n",
        "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
        "                classification = distances.index(min(distances))\n",
        "                self.classifications[classification].append(featureset)\n",
        "\n",
        "            prev_centroids = dict(self.centroids)\n",
        "\n",
        "            for classification in self.classifications:\n",
        "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
        "\n",
        "            optimized = True\n",
        "\n",
        "            for c in self.centroids:\n",
        "                original_centroid = prev_centroids[c]\n",
        "                current_centroid = self.centroids[c]\n",
        "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
        "                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
        "                    optimized = False\n",
        "\n",
        "            if optimized:\n",
        "                break\n",
        "\n",
        "    def predict(self,data):\n",
        "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
        "        classification = distances.index(min(distances))\n",
        "        return classification\n",
        "        \n",
        "colors = 10*[\"g\",\"r\",\"c\",\"b\",\"k\"]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}